{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc01227-6c47-42d9-a42e-0459796990f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datacube_tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "dc = datacube_tools.DATACUBETOOLS()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29d2d0",
   "metadata": {},
   "source": [
    "## Import datacubes from ITS_LIVE V01 to get some useful bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a76e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import directory in which this notebook is stored\n",
    "import os\n",
    "path = os.getcwd()\n",
    "\n",
    "\n",
    "# Get the path of the folder SVD_Code\n",
    "path = re.sub('SVD_Code', '', path)\n",
    "\n",
    "# Set Data path\n",
    "data_path = path + 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbe222",
   "metadata": {},
   "source": [
    "## Import template datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecbc7f-62fe-4f99-8ba8-6c85e60cad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"mid_date\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"mid_date\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import current path \n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "chunks = {'mid_date': 1000, 'x':100, 'y':100}  # chunk for lazy memory loading\n",
    "ds_32607 = xr.open_dataset(f'{data_path}/MalaspinaGlacierCube_32607.nc',chunks=chunks) # Open the main dataset\n",
    "ds_32608 = xr.open_dataset(f'{data_path}/MalaspinaGlacierCube_32608.nc',chunks=chunks) # Open the secondary dataset (smaller in size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99d3e7",
   "metadata": {},
   "source": [
    "## Create a bounding box from these to download the V02 ITS_LIVE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a8f6f2-9551-427c-859a-bfc0984b79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max date of both datasets\n",
    "sdate = np.max([np.max(ds_32608.mid_date.values), np.max(ds_32607.mid_date.values)])\n",
    "sdate = np.min([np.min(ds_32608.mid_date.values), np.min(ds_32607.mid_date.values)])\n",
    "bbox = []\n",
    "\n",
    "# Get bounding box\n",
    "bbox.append(np.min(ds_32607.x.values))\n",
    "bbox.append(np.max(ds_32607.x.values))\n",
    "bbox.append(np.min(ds_32607.y.values))\n",
    "bbox.append(np.max(ds_32607.y.values))\n",
    "\n",
    "import pyproj\n",
    "\n",
    "# Define the EPSG codes for the source and target coordinate reference systems\n",
    "src_crs = 'EPSG:32607'\n",
    "dst_crs = 'EPSG:3413'\n",
    "\n",
    "# Define the transformer from the source CRS to the target CRS\n",
    "transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=True)\n",
    "\n",
    "# Perform the coordinate transformation\n",
    "xmin_3413, ymin_3413 = transformer.transform(bbox[0], bbox[2])\n",
    "xmax_3413, ymax_3413 = transformer.transform(bbox[1], bbox[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07470c46-2d01-4fde-9573-674202d33a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/indexing.py:1617: PerformanceWarning: Slicing with an out-of-order index is generating 5067 times more chunks\n",
      "  return self.array[key]\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/indexing.py:1617: PerformanceWarning: Slicing with an out-of-order index is generating 2353 times more chunks\n",
      "  return self.array[key]\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/xarray/core/indexing.py:1617: PerformanceWarning: Slicing with an out-of-order index is generating 10372 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "def Download_Datacube(var, threshold, bbox, sdate, edate, chunks = 'auto'):\n",
    "\n",
    "    # Get the ITS_LIVE datacube url for the bounding box\n",
    "    urls = []\n",
    "    for i in [0,1]:\n",
    "        for j in [2,3]:\n",
    "            cubefeature, bbox_centrer_point_cubexy = dc.find_datacube_catalog_entry_for_point((bbox[i],bbox[j]), '32607')\n",
    "            urls.append(cubefeature['properties']['zarr_url'])\n",
    "    urls = list(set(urls)) # Remove duplicates\n",
    "\n",
    "    cubes = [] # List to store the datacubes\n",
    "    size = 0\n",
    "    i = 0\n",
    "\n",
    "    # Select the variables to keep\n",
    "    variables_to_keep = [var, 'mid_date', 'x', 'y']\n",
    "\n",
    "    qual = []\n",
    "    dates = []\n",
    "    for url in urls:\n",
    "        # Load indices of slices above the quality threshold\n",
    "        valid = xr.open_dataset(url, engine='zarr').roi_valid_percentage.values\n",
    "\n",
    "        # Grab the time values\n",
    "        t = xr.open_dataset(url, engine='zarr').mid_date.values\n",
    "        \n",
    "        # Create a time mask, based on the validity of layers and the custom date-range\n",
    "        t_mask = np.logical_and(valid>threshold, t>sdate)\n",
    "\n",
    "        qual.append(t_mask)\n",
    "        dates.append(t)\n",
    "\n",
    "\n",
    "        # Open dataset\n",
    "        ds = xr.open_dataset(url, decode_timedelta=False, engine=\"zarr\", consolidated=True, chunks=chunks)[f\"{var}\"]\n",
    "\n",
    "\n",
    "        \n",
    "        # Drop variables not in the list\n",
    "        ds_subset = ds.drop_vars([var for var in ds.variables if var not in variables_to_keep])\n",
    "        \n",
    "        # Apply the mask to the dataset\n",
    "        ds = ds.sel(mid_date=t_mask,\n",
    "                    x=slice(xmin_3413, xmax_3413),\n",
    "                    y=slice(ymin_3413, ymax_3413))\n",
    "        \n",
    "        ds = ds.sortby('mid_date')\n",
    "        ds = ds.drop_duplicates(dim='mid_date')\n",
    "    \n",
    "        \n",
    "        # Resample and calculate the mean every 5 days\n",
    "        cubes.append(ds)\n",
    "\n",
    "\n",
    "    # Assuming your datasets are stored in a list called 'datasets'\n",
    "    common_dates = cubes[0]['mid_date']  # Start with the dates from the first dataset\n",
    "    for ds in cubes[1:]:\n",
    "        common_dates = np.intersect1d(common_dates, ds['mid_date'])  # Find common dates\n",
    "\n",
    "    # Now filter each dataset to keep only the common dates\n",
    "    cubes = [ds.sel(mid_date=common_dates) for ds in cubes]\n",
    "\n",
    "    # Combine the cubes into a single dataset\n",
    "    cubes = xr.combine_by_coords(cubes)\n",
    "\n",
    "    # Rechunk the dataset\n",
    "    cubes = cubes.chunk({'mid_date':1000, 'y': 100, 'x':100})\n",
    "\n",
    "    # Write the dataset on disk\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    write_job = cubes.to_netcdf(f'{data_path}single_threader{var}.nc', compute=False)\n",
    "    with ProgressBar():\n",
    "        print(f\"Writing to {f'{data_path}single_threader{var}.nc'}\")\n",
    "        write_job.compute(scheduler='single-threaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc4289-dfab-4e46-8c76-18da568ce835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to test_single_threadervy.nc\n",
      "[                                        ] | 0% Completed | 242.04 us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 232.75 s"
     ]
    }
   ],
   "source": [
    "# Set data threshold\n",
    "threshold = 40 # Slices with less than 40% of valid data will be discarded\n",
    "\n",
    "# Set the variable to download\n",
    "var = 'vx' # Choose between 'v', 'vx', 'vy'\n",
    "Download_Datacube(var, threshold, bbox, sdate, edate, chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
